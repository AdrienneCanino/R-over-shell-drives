---
title: "R-over-shell-drives Notebook"
output: html_notebook
  df_print: paged
editor_options: 
  chunk_output_type: inline
---

# Front matter
This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r load environment, include=FALSE}
plot(treering) # a base R command
library(tidyverse) # the main library used in this notebook
library(stringi) # a string manipulation library
library(DT) #bring in a library to make nice looking html widget tables in the notebook's output
print("let's get started")
```
Outut from code in this notebook will show in the console or a seperate window.

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

# Introduction 
We have 59 hard drives to explore from Shell (as in the oil company). We want to archive any useful raw data, our first goal being to find useful raw passive acoustic data.

## About the drives
These drives are organized as they were when they were discharged from Shell Oil Company to North Pacific Research Board. Axiom then proposed a small project to extract the passive acoustic data, a 'data archaeology' experiment in rescuing useful scientific data.

That is to say, they're not organized according to any plan, but represent the way the harddrives were used by the scientists during active data management. There's a lot to be learned from them and their organization method, but the goal of the project was to 'rescue' audio files and their contextual metadata. 

These drives were shipped across Alaska and the West Coast to be loaded directly onto Axiom servers, therefore allowing remote access by staff who would attempt the rescue. Axiom has the resulting inventory.

## About the problem
There is no organization or README or particular point person from Shell attached to the project, so contextual information or institutional knowledge about the drives are limited.  
The volume of information we're discussin across these 59 hard drives is large: 145.04 terabytes. So it's a very big system to access and peruse 'by hand' through a GUI file/folder explorer. Equally challenging to explore via single command line exercises like 'ls' and 'find'.  
The types of files on them are varied, from .doc, to .docx, to pdf and wav, and even files without extensions in the name.  

To summarize, we have to use machine methods to search out passive acoustic raw files and their accompanying description files (if any exist).

# Methods
## 1 Create 'dummy' records
  After a lot of time working remotely with the drives, Chris built a python script that ran over the course of a weekend. It 'walked' the directories of all the drives and recorded the filepaths in progressive steps. This code was written in Python3 using the OS library. Included here, but setup for reproducibility and not recommended to run as it did take all weekend.
  
  ```{python}
  import os
  drives=[]
  
  #shelldrives is a text file of drive names made with ls -l >> /home/chris/Documents/shell.scratch/shell.drives
  shelldrives = open("/home/chris/Documents/shell.scratch/shell.drives", "r")
  
  for thing in shelldrives.readlines():
   	drives.append(thing)
  
  dirs_to_ignore = ["System Volume Information","_drive","$RECYCLE.BIN"]
  
  for i in drives:
      drive = i.replace("\n","")
      outfile = "/home/chris/projects/dc/shell.data.rescue/drive.invs/shell."+drive
      f = open(outfile,"a")
      in_dir = "/mnt/shell/"+drive
      print("Starting to inventory "+in_dir)
  
      for (dirpath, dirnames, filenames) in os.walk(in_dir):
          for dirname in dirnames:
              if dirname in dirs_to_ignore: 
                  pass
              else:
                  for g in filenames:
                      f.write(str(os.path.join(dirpath,g))+'\n')
                  for d in dirnames:
                      f.write(str(os.path.join(dirpath,d))+'\n')
  ```
  
  These dummy text files were stored in the `new.invs` directory next to the git repository (because it was too big for github.com to include). They look like this:
  ```
  /mnt/shell/ax29/_drive
  /mnt/shell/ax29/System Volume Information
  /mnt/shell/ax29/fw
  /mnt/shell/ax29/System Volume Information/EfaData
  /mnt/shell/ax29/fw/chukchi
  /mnt/shell/ax29/fw/chukchi/2011-summer
  /mnt/shell/ax29/fw/chukchi/2011-summer/CLN90B
  /mnt/shell/ax29/fw/chukchi/2011-summer/KL01
  /mnt/shell/ax29/fw/chukchi/2011-summer/CLN120B
  /mnt/shell/ax29/fw/chukchi/2011-summer/WN20
  /mnt/shell/ax29/fw/chukchi/2011-summer/CL05
  /mnt/shell/ax29/fw/chukchi/2011-summer/BG01
  /mnt/shell/ax29/fw/chukchi/2011-summer/CL20/acoustic-analysis-20111123T124920.log
  /mnt/shell/ax29/fw/chukchi/2011-summer/CL20/030
  /mnt/shell/ax29/fw/chukchi/2011-summer/CL20/Temperature_Channel
  /mnt/shell/ax29/fw/chukchi/2011-summer/CL20/acoustic-analysis-20111123T124920.log
  /mnt/shell/ax29/fw/chukchi/2011-summer/CL20/030
  /mnt/shell/ax29/fw/chukchi/2011-summer/CL20/Temperature_Channel
  /mnt/shell/ax29/fw/chukchi/2011-summer/CL20/030/Unit030chukchi2011-e1d282a3.E1D282A3.Chan_1-24bps.1311562201.2011-07-25-02-50-01.wav
  /mnt/shell/ax29/fw/chukchi/2011-summer/CL20/030/Unit030chukchi2011-e1d282a3.E1D282A3.Chan_1-24bps.1316967601.2011-09-25-16-20-01.wav
  ```
  

## 2 Read 'dummy' records into dataframes for tidy manipulation
Now to create a list of the files, then loop through the list and read the lines and lines of text into an object in R, preferable a dataframe I can manipulate with common tools, rather than be dependent on string manipulation constantly.

```{r loop through read command}
#make sure wd is local git repo
#setwd()
files <- list.files(path="../new.invs")

##create column header for the dfs I'm going to make
columnNames = c("path","directory","subdirectory1", "subdirectory2","subdirectory3","subdirectory4","subdirectory5","subdirectory6","subdirectory7","subdirectory8","subdirectory9","subdirectory10" )

#setup objects I'll use in temporary parts of the loop
dat <-  NULL
drives_lst <- NULL

#loop through the read_lines command
for(f in files){
  #setup file path for reading
  pth <- capture.output(cat("../new.invs/",f, sep=""))
  print(pth)
  
  #read in the lines to dataframe with base r read function
  dat <- read.delim(file=pth, sep="/",
                col.names = columnNames, header = FALSE, comment.char="",
                blank.lines.skip=FALSE, fill =TRUE)
  
  #make a context specific name for that DF, assign it
  nam <- paste("df", as.character(f), sep = "_")
  print(nam)
  assign(nam, dat)
  
  #make a list of DF names to iterate through
  drives_lst <- append(drives_lst, nam)
  
}

#Turn that list of names into object listing the call-able dataframes
new_drives_lst<- lapply(drives_lst, get)
```

These dataframes broke down the lines of paths in the text file by the `\` seperator, so I can manipulate the information by subdirectory. 
For example:
```{r subdirectory summary example}
# a summary by count grouped by subdirectory4, or: /mnt/shell/ax81/subdir/sub_subdir/sub_sub-subdir
df_shell.ax81 %>%
  group_by(subdirectory4) %>%
  summarise(n=n()) %>%
  view()

```

## 3 Include full file paths in these dataframes
But these dataframes are much more useful with one column that does maintain the real filepaths, so to speak, as those will be the most useful outputs for actually copying and moving the audio files into curated batches.

To do this I'll re-make the dataframes with the filepath column.

```{r loop to create united filepaths col}
# make loop variables
i <- 1
path_dfs_lst <- NULL
dat <- NULL
nam <-  NULL

#loop through unite command from dplyr library
for(thing in new_drives_lst){

  #re-assign dataframe with united colum
  dat<- 
    as_tibble(new_drives_lst[[i]])%>% 
    #mutate_all(as.character) %>%
    unite(col = "file_path", 1:12, remove = FALSE, na.rm = T, sep = "/")
  
  dat$file_path <-trimws(dat$file_path, which="right", whitespace="/")
  
  #rename it and list it
  nam <- paste("paths_df", files[i], sep = "_")
  print(nam)
  assign(nam, dat)
  path_dfs_lst <- append(path_dfs_lst, nam)
  
  #iterator
  i <- i+1
  
}

#make that list of names a list of callable objects
new_path_dfs_lst <- lapply(path_dfs_lst, get)
```

Now I do not need all of these data frames so I'll use the list of names from creating the first dataframes to remove those 59 objects and free up space in my local machine.

```{r remove earlier DFs }
rm(list=drives_lst)

```

## 4 Find which drives have wav, csv, or xcl files
###WAVS
Now I'll make a loop to find any drives that have wav files present.
```{r find and write out wav file loop}
#set iterator for in the loop, what I am actually going to iterate through is the list of drives I have stored in the list-object 'files'
i <- 1

#loop through the callable-objects list of dfs
for(df in new_path_dfs_lst){
  #use tidyverse stringr 'which' command to match a regular expressions pattern
  wav_index <-str_which(df$file_path, regex(".wav$", ignore_case=TRUE))
  
  #get it's length
  len <- length(wav_index)
  #a print command to check, printing a call from a list will make it take a little bit of time
  print(c(files[i],"wav files found:",len))
  
  #if there's any matches (a length over 0) write out those file paths to the outputs folder
  if(len>0){
    dir.create(path=paste("outputs/",files[i], sep=""))
    df_wav <- df[wav_index,]
    df_wav <-  df_wav %>% distinct()
    write_lines(df_wav$file_path, file=paste("outputs/",files[i],"/wav-files_",files[i],".txt", sep=""))
  }
  i <- i+1
}
```

###CSVs
Now I'll look for csvs, particularly the deploymentINFO.csv we know exists in at least a dozen places.
```{r find and write out csv file loop}

#set iterator for in the loop, what I am actually going to iterate through is the list of drives I have stored in the list-object 'files'
i <- 1

#loop through the callable-objects list of dfs
for(df in new_path_dfs_lst){
  #use tidyverse stringr 'which' command to match a regular expressions pattern
  csv_index <-str_which(df$file_path, regex(".csv$", ignore_case=TRUE))
  
  #get it's length
  len <- length(csv_index)
  #a print command to check, printing a call from a list will make it take a little bit of time
  print(c(files[i],"csv files found:",len))
  
  #if there's any matches (a length over 0) write out those file paths to the outputs folder
  if(len>0){
    #check for the dir existing already, make it if it doesn't
    ifelse(!dir.exists(paste("outputs/",files[i],sep="")), dir.create(paste("outputs/",files[i],sep="")), print("Folder exists"))
    #make the dataframe, write out the file paths column from it with the right relative filepath
    df_csv <- df[csv_index,]
    df_csv <-  df_csv %>% distinct()
    write_lines(df_csv$file_path, file=paste("outputs/",files[i],"/csv-files_",files[i],".txt", sep=""))
  }
  i <- i+1
}
```

###Excel
In an effort to find more deployment info spreadsheets, let's look for excel style files.

```{r find and write out xls* file loop}
#set iterator for in the loop, 29 because my drives start their numbering at 29
i <- 1

#loop through the callable-objects list of dfs
for(df in new_path_dfs_lst){
  #use tidyverse stringr 'which' command to match a regular expressions pattern
  xls_index <-str_which(df$file_path, regex(".xls*$|.xlsx$", ignore_case=TRUE))
  
  #get it's length
  len <- length(xls_index)
  print(c(files[i],"excel files found:",len))
  
  #if there's any matches (a length over 0) write out those file paths to the outputs folder
  if(len>0){
    #check for the dir existing already, make it if it doesn't
    ifelse(!dir.exists(paste("outputs/",files[i],sep="")), dir.create(paste("outputs/",files[i],sep="")), print("Folder exists"))
    #make the dataframe, write out the file paths column from it with the right relative filepath
    df_xl <- df[xls_index,]
    df_xl <-  df_xl %>% distinct()
    write_lines(df_xl$file_path, file=paste("outputs/",files[i],"/xcl-files_",files[i],".txt", sep=""))
  }
  i <- i+1
}
```
There are 7 drives with excel files in them, sometimes, over 1000 evidently?

### Results, in sum, of what kinds of files where
```{r make a summary table }
summaryDF <-  as.data.frame(matrix(0, nrow=1, ncol=4, dimnames=list(NULL,c("drive", "WAVS", "CSVS", "EXCELS"))))
i <- 1
 #loop through the callable-objects list of dfs
 for(df in new_path_dfs_lst){
   #use tidyverse stringr 'which' command to match a regular expressions pattern
   xls_index <-str_which(df$file_path, regex(".xls*$|.xlsx$", ignore_case=TRUE))
   csv_index <-str_which(df$file_path, regex(".csv$", ignore_case=TRUE))
   wav_index <-str_which(df$file_path, regex(".wav$", ignore_case=TRUE))
   
   #get it's length
   len_xl <- length(xls_index)
   len_csv <- length(csv_index)
   len_wv <- length(wav_index)
   
   #a print command to check, printing a call from a list will make it take a little bit of time
   #print(c(files[i],"wav files found:",len_wv,"csv files found", len_csv, "excel files found", len_xl))
   
   summaryDF[nrow(summaryDF)+1,] =c(files[i], len_wv, len_csv,len_xl)
   
   #iterator
   i <-  i+1
           
 }
```

```{r nice summary table}
print(summaryDF)

write_csv(summaryDF, file="summaryDF.csv")
```

###optional cleanups

```{r optional cleanups}
rm(
  csv_index,
  wav_index,
  xls_index,
  xlsx_index,
  pth,
  nam,
  len_csv,
  len_wv,
  len_xl,
  i,
  thing
  #summaryDF
  )
```
There may be some warnings. Comment out objects as you see fit.


## 5 Copy the files to local machine
Now that  we have the file paths to where the deployment information sheets are, I can copy them from where they exist in the remote drives and store them in a directory of this git repo (AKA my local machine).  I've maintained a drive hierarchy so that related files to eachother are still maintained in a subdirectory of `outputs` withthe name of the drive folder, so I could make a loop to do these things too.

What I want is all csv files, so I can clean any deployment info files.
The following code is representative of how I did this, but not setup in this notebook for reproducibility.

```{label=shell commands for copying files from remote}
#Pull together a list of all the files I want (all of them)
cat *.txt > all-shellax29-files.txt

#also aim for or be in the 'outputs' directory of the repo where this notebook/these files were stored

#use command from rsync library to fetch and copy the files as listed in these locations - CSV only for now
# --no-relative flag is what removes the folder/subfolder structure of the files. Otherwise sit will copy everything in the structure it's in. Useful for context, not for getting all the files I want.

rsync -ar --no-relative / --files-from=/home/adrienne/Documents/R-over-shell-drives/outputs/shell.ax29/all-ax29-csv.txt /home/adrienne/Documents/R-over-shell-drives/outputs/shell.ax29

```

Rinse and repeat.
I know from my summary DF that not all the drives have CSVS.

## 6 Use the csv files to make cleandeploymentinfo tables
### Find and make list of csv drive
``` {r making a deployment  csv drive list}
#make a list to iterate through
deploydflst <- NULL
i <- 1
testlst <-  NULL
fls <- list.dirs(path="./outputs")
#operate the loop in the folder where the csvs exist DO THIS PART BY HAND
#setwd("./ouputs")
#nested for loops?

for(f in fls){
  pth <- paste("./",f,sep="")
  files <- list.files(path=pth, pattern ="*.csv")
  #use this list of csvs to create deployment info files
    for (f in files){
      if(str_detect(f, "deployment"))
      #look for number of cols
      wideness <- count.fields(f,sep=",", comment.char = "",skip=5,blank.lines.skip = FALSE)
      wideness <- max(wideness)
      #make dataframe
      dat <-read.table(f, 
                       header=F, sep="," ,
                       blank.lines.skip = FALSE, comment.char="", 
                       colClasses = c("character","character", "character"),
                       col.names = 1:wideness) #read table
      nam <- paste("deployDF", i, sep = "_")
      print(nam)
      assign(nam, dat)
      #make a list to iterate through
      testlst[[i]] <- nam
      i <- i+1
      else(print(f,"No deployment csv found"))
    }
  }





#that list isn't what I want, it's just names without pointing to the object DF in R, but this is:
deploydflst <- lapply(testlst, get)

#clean up what I don't need from that loop
rm(wideness, dat, nam,f)
```


### Use the csvs to make clean deployment files

## 7 Use the wav filepaths to get the WAV files copied locally
### First determine the files related to a particular deployment

### Now write a list of those wav filepaths and store it next to the deploymentinfo sheet

### Now carefully copy those filepaths from the remote drives to my local machine
# Results
# Discussion
# Next
# Back matter